---
title: "bilan fin décembre"
author: "Team PAT"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r}
# packages à charger
library(purrr)
library(easyCODA)    # Greenacre 
library(FactoMineR)

library(tidyverse)
library(dplyr)

library(Factoshiny)
library(ggplot2)
library(ggrepel)
library(RColorBrewer)
library(plotly)
library(tidyr)

```

```{r}
# chargement des données des autres RMD

load(file = "data/clust.RData")     # coordonnéees des mots = formes fortes 
load(file = "data/documents.RData") # comptage des lemmes dans les doc
load(file = "data/vocab.RData")     # identification des mots 
load(file = "data/mfa.df.final.RData")
load(file = "data/res.lemmat.RData")
load(file = "data/res.hcpc.RData")
load(file = "data/clust.RData")
load(file = "data/res.mfa.RData")

```

# Objectifs 

A partir des formes fortes obtenues par MFA, on souhaite extraire les compositions des descriptions des PAT. On va comparer différentes procédures pour aboutir à la classification des PAT, en commençant par la caractérisation de nos PAT à travers la fréquence d'apparition de chaque terme de chaque topic dans les descriptions des PAT tokenisées.

# Formes Fortes 

Voici les formes fortes identifiées (voir doc 12.12) :

```{r}
source(file = "functions/plot_clust.R")

plot_clust(res.hcpc, res.mfa, mfa.df.final)
```

Les formes fortes ne sont pas toutes composées du mêmes nombre de mots :

```{r}
clust %>% 
  group_by(clust) %>% 
  summarise(sum = n())
```

Cela caractérise le fait que certains thèmes sont bcp plus riches en mots. Faut il considérer le même nombre de mots par thème ? 

En effet, le thème 3 , qui a le plus de mots, a peut-être plus de chances d'être un thème que l'on retrouve plus au niveau fréquence dans les PAT (ce que l'on ne confirme pas à posteriori ). 
On prend note de cela mais pour l'instant on va conserver l'ensemble des termes composant nos topics fortifiés. 


## Association des formes fortes - axes thématiques
```{r}
# à faire 
# + description des PAT (à demander)
```


# Construction des df

On va "reconstituer" dans df_textes chaque description de PAT à travers les tokens de ces descriptions.
```{r}
df_textes <- imap_dfr(documents, function(mat, nom_doc) {
  
  # index des mots
  idx <- mat[1, ]
  
  # fréquences
  freq <- mat[2, ]
  
  # conversion index -> mots
  mots <- vocab[idx]
  
  # répéter les mots selon la fréquence
  texte <- paste(
    rep(mots, times = freq),
    collapse = " "
  )
  
  tibble(
    doc = nom_doc,
    texte = texte
  )
})

df_textes

```



On crée le data frame "df_resultat.final" qui correspond à la table de comptage des termes de nos topics fortifiés (par topic) par PAT.
```{r}
df_mots <- tibble(mot = rownames(mfa.df.final)) # une colonne pour les mots 

df_cross <- crossing(df_mots, df_textes) # tous les croisement des mots (132) avec tous les textes (360)

df_count <- df_cross %>%  mutate(occurrence = str_count(texte, fixed(mot)))  # comptage des occurences de chaque mot dans le texte 

df_resultat <- df_count %>%          # df avec autant de lignes que de mots, et de colonnes que de PAT et au croisement les occurences des mots (issus des formes fortes)
  select(mot, doc, occurrence) %>%
  pivot_wider(
    names_from = doc,
    values_from = occurrence
  )

rm(df_mots)
rm(df_cross)
rm(df_count)

# on trie les mots (lignes) du doc clust par ordre alphabétique pour ensuite récupérer les clusters des mots 
clust <- clust %>% arrange(by = word) # ⚠ important avant le merge de la colonnes clust !!
  
df_resultat$cluster <- clust$clust

df_resultat.final <- data.frame(df_resultat %>%
  group_by(cluster) %>%
  summarise(across(where(is.numeric), sum, na.rm = TRUE)))


# sum(is.na(df))
# 
# max.col(df)
# 
# colnames(df)[max.col(df)]
# 
# 
# 
# summary(colSums(df_resultat.final[,-1]))

df_resultat.final <- df_resultat.final[,-1]

```

# Filtrage PAT nb termes FF

Filtre pour supp les docs qui ont pas assez de mots qui se retrouvent 

On veut filtrer les PAT qui n'ont pas assez de mots issus de nos formes fortes, ou placer la limite ? 
On trace l'histogramme du nombre total de termes issus de l'ensemble de nos topics (formes fortes). On avait fixé la limite à 10 termes minimum issus des formes fortes pour que le PAT soit conservé pour la classification 

```{r}
hist(colSums(df_resultat.final), breaks = 200, xlim = range(0:150))
abline(v = 10, col = "blue4", lwd = 3)
```

On va créer le data frame filtré contenant uniquement les PAT avec plus de 10 termes issus des formes fortes. 
```{r}
df_filtre <- as.data.frame(df_resultat.final[, colSums(df_resultat.final) >= 10])
```


# Table de Fréquence
On va créer le tableau contenant les fréquences de chaque topic à partir de nos comptages.
```{r}
df <- data.frame(apply(df_filtre,2,function(x) x/sum(x)*100))
df[df=="NaN"] <- 0
```

On va transposer le df pour obtenir un tableau de données avec 6 colonnes (les titres sont issus des noms extraits par extraction des variables latentes avec NailR) et autant de lignes que de PAT après sélection (PAT ayant une description + au moins un certain nb de caractères dans la description et au moins 10 termes issus de nos formes fortes)
```{r}
df <- data.frame(t(df))
colnames(df) <- c("Gouvernance", "Education/Justice Sociale", "Territoires" , "Environnement","Secteur agricole", "Economie alimentaire")
rownames(df) <- as.numeric(gsub("text", "", rownames(df)))

df <- df[order(as.numeric(rownames(df))), ]
```


# PAT Purs 

Comment se répartissent les vecteurs de composition des PAT en 6 topics ? 

```{r}
summary(df)
```


```{r}
plot <- df %>%
  pivot_longer(cols = everything(),
               names_to = "Groupe",
               values_to = "Valeur") %>%
  ggplot(aes(x = Groupe, y = Valeur)) +
  geom_boxplot(fill = "skyblue") +
  theme_minimal() +
  labs(title = "Distribution par groupe",
       x = "Groupe",
       y = "Valeur") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot + ylim(0, 50)

# from gemini request 
# 12.12.25 Rmd
```

On veut classifier nos PAT selon ces vecteurs de proportion de nos 6 topics. 

⚠️ Ces vecteurs de proportions sont construits sur l'occurence des mots représentatifs de chaque topic dans chacun des PAT. Ces mots sont les termes ayant les indices frex les plus forts, consolidés par de nombreuses éxecutions de la LDA, puis filtrés pour supprimer les mots n'apparaissant que dans un seul topic d'une seule LDA, puis regroupés dans des formes fortes par classification hiérarchique précédée d'une MFA. 


On s'intéresse aux PAT qui sont les plus purs, c'est à dire qui prennent la valeur maximale pour un des topics. On va essayer de comprendre la solidité de notre mesure de fréquence en comparant ces PAT avec leur description. 

On pourrait utiliser ces PAT comme exemple pour fournir à un modèle de langage de l'information supplémentaire liée à chaque topic pour en extraire le nom de la variable latente ensuite. 

```{r}
summary(df)

df.t = data.frame(t(df)) 
df.t[,max.col(df.t)] # on cherche les textes les plus purs de chaque topic et on affiche leur profils de répartition des topics 
```

Analyse de ces PAT "purs" 
=> qualitative : à la main, en comparant avec la base



=> qualitative : avec modèle de langage 
```{r}

```



# Comparaison de deux approches de classification 

## Méthode Greenacre, classif de vecteurs de fréquences

### Préparation des données 

Il faut gérer les fréquences égales à zéro. Dans la méthode décrite par Greenacre [Greenacre, 2019 ](https://www.researchgate.net/publication/327387401_Compositional_Data_Analysis_in_Practice), on utilise des transformations logarithmiques avant de réaliser une classification par AFC. 

Pour réaliser les transformations logarithmiques, on doit d'abord traiter la problématique des zéros. Il existe plusieurs packages qui réalisent ce traitment, mais nous n'avons pas réussi à les télécharger donc nous avons testé nous mêmes plusieurs approches : 

```{r}
# zeros ?

100*sum(df==0)/(nrow(df)*(ncol(df))) # % de zéros
```

On va coder nous mêmes le remplacement des zéros par BDL = remplacement de chaque zéro par 2/3 de la valeur minimale de chaque colonne 
```{r}
# df[df==0] <- 1000
# df[minRow(df),]

min <- apply(df, 2, function(x) min(x[x != 0]))
min <- (2/3)*min

# replace les zero par la valeur 
for (i in seq_along(min)) {
  df[df[, i] == 0, i] <- min[i]
}

summary(rowSums(df)) # du coup, toutes les fréquences ne sommes plus à 100

# on recalcule nos fréquences 
df <- t(apply(df,1,function(x) (x/sum(x))*100))


######### nb : easyCODA::LRA => compute une CLR 

res.alr <- CLR(df, weight = F)
# res.pca <- easyCODA::PCA(res.alr, nd = 5)
# easyCODA::PLOT.PCA(res.pca)

res.ward <- easyCODA::WARD(res.alr, weight = F)

```

Remplacement par une petite valeur: 
```{r}

```

### Transformations logarithmiques

On réalise la transformation par LRA, puis on réalise une PCA (en fait une AFC) sur le tableau de données compos


```{r, ignore = T}
######################################
# PCA sur le res de CLR 
res.pca <- FactoMineR::PCA(res.alr$LR, ncp = 3)
barplot(res.pca$eig[,2])

res.hcpc <- HCPC(res.pca, nb.clust = 3)

res.hcpc$desc.var
# Variables => donc topic sous ou sur représentés dans le cluster

# clust1 :   Gouvernance, démographie, env   
# clust2 :   Gouvernance, secteur agricole, justice sociale 
# clust3 :   Eco alim, secteur agricole, env 

res.hcpc$desc.ind 
# clust1 : 441        36       457       318       211 
# clust2 : 443       122       341       323       269 
# clust3 : 178       174       167       181       319 
```

PCA sur le tableau de données brut
```{r}
res.pca.brut <- FactoMineR::PCA(df)
barplot(res.pca.brut$eig[,2])
res.hcpc.brut <- HCPC(res.pca.brut, nb.clust = -1)

# test avec Nb opt = 3 clust 
HCPC(res.pca.brut, nb.clust = 3)$desc.var
# clust 1 : eco alim + environnement => production alimentaire verte 
# clust 2 : secteur agri + alim durable et sociale (env - ) => soutien agricole et justice sociale 
# clust 3 : démo + alim durable + gouvernance (eco alim -) => soutien à la population 

HCPC(res.pca.brut, nb.clust = 3)$desc.ind

# test avec NB.CLUST = 6
HCPC(res.pca.brut, nb.clust = 6)$desc.var
# clust 1 : eco alim 
# clust 2 : secteur agri 
# clust 3 : environnement
# clust 4 : gouvernance
# clust 5 : alim durable et sociale 
# clust 6 : démo 

```

## AFC sur table de comptage 

CA sur table de comptage
```{r}
df.ca <- data.frame(t(df_filtre))
colnames(df.ca) <- colnames(df)
rownames(df.ca) <- as.numeric(gsub("text", "", rownames(df.ca)))
df.ca <- df.ca[order(as.numeric(rownames(df.ca))), ]

res.ca <- FactoMineR::CA(df.ca)
plot.CA(res.ca, invisible = "row")

res.hcpc.ca <- HCPC(res.ca, nb.clust = -1)
res.hcpc.ca$`1` # eco alim, env
res.hcpc.ca$`2` # agri + alim coll durable 
res.hcpc.ca$`3` # gouvernance 
```



# Score d'adéquation avec LLM

Une autre méthode pourrait être d'essayer d'utiliser des modèles de langage pour calculer un score (=> et au final un vecteur) d'association entre chaque description de PAT et les topics consolidés. C'est une approche que l'on peut utiliser pour comparer aux résultats obtenues par les méthodes de classifications réalisées précédemment. 



