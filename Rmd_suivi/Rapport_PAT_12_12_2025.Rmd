---
title: "Bilan analyse textuel - LDA - Projet PAT"
author: "Erwan Gouhier, Anna Mathieu"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    toc_float: true
    self_contained: true
    df_print: paged
    highlight: tango
    tabset: true
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r packages, include=F}
library(topicmodels)
library(tidytext)
library(stringr)
library(quanteda)
library(udpipe)
library(tidyverse)
library(dplyr)
library(remotes)
library(readxl)

library(FactoMineR)
library(Factoshiny)
library(ggplot2)
library(ggrepel)
library(RColorBrewer)
library(plotly)
library(tidyr)

library(stm)

library(NaileR)

library(forcats)
library(ggforce)
```

# Objectifs :

-   Nous essayons de d√©terminer le nombre optimal de topic √† retenir, k, notre
    principal hyperparam√®tre.
-   Nous essayons d'obtenir des formes fortes de notre topic modelling pour
    consolider nos topics √† travers les termes qui les caract√©risent

Nous avions des soucis avec la LDA la semaine derni√®re li√©s √† la
reproductibilit√© (seed non fix√©s) et li√©s aux m√©triques d'√©valuation des topics.
La fonction utilis√©e ne permettait pas de r√©cup√©rer plus que les coefficients
beta (et les mots ayant le coeff beta le plus √©lev√© pour chaque topic).

La fonction *stm* permet de fixer une seed et de r√©cup√©rer plus de m√©triques,
comme nous allons le d√©velopper plus tard.

La pr√©paration des donn√©es est l√©g√®rement diff√©rentes, il faut pr√©parer le
vocabulaire (ensemble de tous les mots) ainsi que les documents (noms des
documents =\> n¬∞ des PAT).

# Pr√©paration

```{r}
# Pr√©paration des documents

# Importation de res lemmat (voir Rapport du 04-12-25)
load("data/res.lemmat.RData")

# step 1 : Create the Vocabulary
vocab <- unique(res.lemmat$lem.f) # => liste unique de mots 
head(vocab)

# step 2 :  Create a Word-to-Index Mapping
# on va associer les num√©ros des mots dans le vocabulaire  (chaque mot prend un numero)
word_to_idx <- setNames(seq_along(vocab), vocab)
head(word_to_idx)

# Step 3: Count Word Occurrences per Document
# Group by document and count occurrences of each word
doc_word_counts <- res.lemmat %>%
  group_by(doc, lem.f) %>%
  summarise(count = n(), .groups = 'drop')
# we count the frequency of each word in each document 
head(doc_word_counts)

# step4 4 : create document list 
# Get unique documents in order => liste des documents 
unique_docs <- unique(res.lemmat$doc)
head(unique_docs)  # correspondent aux lignes de la BDD qui ont une description (ex : la n¬∞3 n'en a pas)

# Create the documents list
documents <- lapply(unique_docs, function(current_doc) {
  # Filter words for this document
  words_in_doc <- doc_word_counts %>%
    filter(doc == current_doc)
  
  # Convert words to indices
  indices <- as.integer(word_to_idx[words_in_doc$lem.f])
  counts <- as.integer(words_in_doc$count)
  
  # Create 2-row matrix: row 1 = indices, row 2 = counts
  matrix(c(indices, counts), nrow = 2, byrow = TRUE)
})
# on va cr√©er une liste qui contient autant d'√©l√©m√©nts qu'il n'y a de textes, dans chaque √©l√©ment, il y a les identifiants des mots qui sont cit√©s dans le vocabulaire (le n¬∞ du mot ) et sa fr√©quence d'apparition dans ce texte

# Name the documents (optional but recommended)
names(documents) <- unique_docs

documents[[1]][1,1] # le premier mot du text 1 (par ordre alphabetique) est le num√©ro 135
vocab[135]
documents[[1]][2,1] # il apparait une seule fois dans ce doc 


# Step 5 : prep with prep documents
out <- prepDocuments(documents, vocab, 
                     lower.thresh = 1,  # remove words appearing in only 1 doc
                     upper.thresh = Inf)

documents <- out$documents
vocab <- out$vocab

load("data/documents.RData")
save(out,file="data/out.RData")
# save(documents, file = "data/documents.RData")
# save(vocab, file = "data/vocab.RData")

```

# Mod√®le et m√©triques

On va ensuite cr√©er une fonction qui cr√©√© le mod√®le de topic modelling avec la
fonction stm.

```{r}
lda.model <- function(k, seed) {
  topic_model<-stm(documents, 
                   vocab,
                   K=k, verbose=FALSE, init.type = "LDA", 
                   seed = seed)
  
  return(topic_model)
}
```

Exemple de fonctionnement de la fonction et des sorties :

```{r, eval = F}
modeltest <- lda.model(k = 9, seed = 1234)
summary(modeltest)

```

Une premi√®re m√©thode √† laquelle nous avons pens√© consistait √† afficher un
graphique √† la mani√®re des valeurs propres en ACP pour d√©cider du nombre de
topics que l'on choisit pour notre LDA. On a cr√©√© une fonction qui prend en
compte 2 arguments (nstart et nend), qui correspondent aux valeurs minimum et
maximum du nombre de topics qu'on fixe dans notre LDA, la fonction teste pour
toutes les valeurs de k comprises dans cet intervalle.

```{r, eval = F}
#On charge le simple triplet matrix que nous avions fait dans le rapport du 04-12
stm <- load(file="data/stm.RData")
```

Pour ce qui est de la coh√©rence mesur√©e dans les topics avec la fonction
`topic_coherence`, elle mesure √† quel point les termes d'un m√™me topic
apparaissent ensemble.

Pour ce qui est du calcul, pour chaque mot d'un topic, on va calculer un rapport
entre :\
- le nombre de co-occurrences de deux termes dans tous les documents\
- le nombre d'occurrences d'un des termes

Le calcul final pour obtenir la coh√©rence est le suivant :

$$
\text{UMass-Coherence} = \sum_{m=2}^{M} \sum_{l=1}^{m-1}
\log \left(
\frac{\text{df}_{m,l} + 1}{\text{df}_{l,l}}
\right)
$$

On fait la somme des logs de fr√©quences de co-occurrences pour chaque terme d'un
topic. Les √©l√©ments du calcul sont les suivants :\
- `df[m,l]` : nombre de documents o√π les mots $l$ et $m$ d'un th√®me apparaissent ensemble ,on ajoute +1 pour √©viter un log de 0 \

- `df[l,l]` : nombre de documents contenant le terme $l$, auquel 

Pour l'algorithme en lui-m√™me, si l'on prend une matrice carr√©e de dimensions
$M \times M$, le calcul se fait sur la **partie triangulaire inf√©rieure** de la
matrice.

Pour donner une d√©finition plus g√©n√©rale d'un score de coh√©rence :

> "In general, a coherence score quantifies the degree of semantic
> interconnection among words within a given topic."\
> ‚Äî Farea et al., 2024

```{r, eval = F}
# install.packages("topicdoc")
library(topicdoc)
library(tidyverse)
load(file = 'data/stm.RData')

#Fonction visant √† produir un graph montrant la coh√©rence moyenne des topicss propos√© √† la sortie d'une LDA entre 2 valeurs du nombre de topics
# nstart : nombre de topics minimum
# nend : nombre de topics maximum
# La fonction a un pas de 1 pour les valeurs de k, il est donc recommand√© de ne pas mettre des valeurs trop √©loign√©es 

coherence_graph <- function(nstart,nend){
  L <- as.data.frame(matrix(nrow=nend-nstart+1,ncol=2))
  colnames(L) <- c("k","min_coherence")
  L$k <- nstart:nend
  
  for (k_topic in nstart:nend){
    lda_model <- LDA(stm, k = k_topic, method = "Gibbs",
                     control = list(seed = as.integer(800)))
    
    L$min_coherence[k_topic-nstart+1] <- min(topic_coherence(lda_model,stm))
  }
  return(L)
}

coherence <- coherence_graph(2,5)

coherence %>% ggplot(aes(x=k,y=min_coherence)) +
    geom_line() +
    geom_point() +
  ggtitle("Coh√©rence moyenne des topics en fonction du nombre de topics")
```

On r√©cup√®re les 7 mots qui ont les scores les plus √©lev√©s par TOPIC pour les
m√©triques suivantes : - coefficients beta : rappel =\> probabilit√©
d'appartenance d'un terme dans un topic

-   FREX : indice de fr√©quence exclusivit√© =\> moyenne harmonique pond√©r√©e dans
    laquelle le rang du mot est une combinaison de sa fr√©quence et de son
    exclusivit√©. La formule est :

$$\text{FREX}_{f,k} = \left( \frac{w}{\text{ECDF}_{\varphi,k}(\varphi_{f,k})} + \frac{1-w}{\text{ECDF}_{\mu,k}(\mu_{f,k})} \right)^{-1}$$

avec :

-   k : le topic
-   w : param√®tre de poids (% associ√© √† l'exclusivit√© et 1-w = % associ√© √† la
    fr√©quence), par d√©faut w= 0.5
-   la puissance -1 est une moyenne harmonique prenant en compte les deux termes
    suivants :

terme 1 - ECDF = fonction de r√©partition empirique cumulative des fr√©quences des
mots d'un th√®me - $\varphi_{f,k}$ = fr√©quence du mot f dans le topic k

terme 2 : - $\mu_{f,k}$ : exclusivit√© d'un mot f dans un topic k - fonction de
r√©partition empirique cumulative des exclusivit√© dans le topic k

Cet indice met l'accent sur des mots typiques et plus exclusifs des th√®mes. La
fr√©quence de certains termes tr√®s g√©n√©riques pr√©sents dans le corpus et qui se
retrouvent dans de nombreux th√®mes (alimentaire, alimentation, territoire). Les
mots avec le score le plus √©lev√© sont ceux qui sont √† la fois assez fr√©quents et
√† la fois assez exclusifs √† un th√®me [Bischof et al,
2012](https://icml.cc/2012/papers/113.pdf)

On va ensuite cr√©er une fonction qui va r√©cup√©rer les mots qui caract√©risent le
plus nos topics extraits.

=\> il faut donc choisir les m√©triques qui nous int√©ressent :

```{r}

topic.extraction <- function(topic_model) {
  
  # r√©cup√©rer les mots avec les indices FREX (Fr√©quence exclusivit√©) les plus forts 
  frex <- data.frame(t(summary(topic_model)$frex))
  
  # beta <-  data.frame(t(summary(topic_model)$prob)) # et les scores beta les plus √©lev√©s 
  
  # cr√©er liste_mots
  # liste_mots <- rbind(frex,beta)
  colnames(frex) <- paste0("topic",seq(from=1,to=k))
  
  list <- sapply(frex, paste, collapse = " ")
  list <- str_split(list, pattern = " ")
  
  return (list)
}


topic.extraction2 <- function(models, nb.frex, frex.threshold = 0.5) {
      # r√©cup√©ration des FREX 
  words_frex <- list()
  
  for (model in seq_along(models)){
    frex_mat <- t(stm::labelTopics(models[[model]], n = nb.frex, frexweight = frex.threshold)$frex)
    # n: choix du nombre de frex , frexweight : optimisation frequence / exclusivit√©
    
    words_frex[[model]] <- vector("list", ncol(frex_mat))
    
    for ( topic in seq_len(ncol(frex_mat))  ) {
      words_frex[[model]][[topic]] <- frex_mat[,topic]
    }
  }
   return(words_frex)
  
  
}
```

Extraction des termes de chaque th√®me avec Beta et frex et montrer que frex est
plus discriminant :

```{r, eval = F}

# On utilise NaileR pour faire une extraction automatique de nos variables latentes

# avec FREX

# dataframe frex 
frex.df <- data.frame(topic.extraction(modeltest))
colnames(frex.df) <- paste0(seq(from=1,to=9))
frex.df.pivot <- pivot_longer(data = frex.df, names_to = "topic", values_to = "word", cols = 1:9)

cat(gemini_generate(nail_textual(dataset = frex.df.pivot, num.var = 1, num.text = 2,
                  
                  introduction = "A study on Territorial food systems is done and we want to find the topics discussed in the description of each Territorial food system, we use topic modelling and latent dirichlet allocation. Voici les mots qui ont les indices frex (fr√©quence exclusivit√©) les plus √©lev√©s pour chaque topic.",
                  request = "We want to automatically put a name on those topics based on the words caracterising them. Please give a name to each topic suming up most of the words caracterising it. Do it in French. Only give la Liste des Noms de Groupe Attribu√©s",, 
                  isolate.groups = F, drop.negative = T, generate = F)
)
)
```

Avec uniquement les scores beta : 
```{r, eval = F}
# avec scores BETA
beta.df <-  data.frame(t(summary(modeltest)$prob))
colnames(beta.df) <- paste0(seq(from=1,to=9))
beta.df.pivot <- pivot_longer(data = beta.df, names_to = "topic", values_to = "word", cols = 1:9)


cat(gemini_generate(nail_textual(dataset = beta.df.pivot, num.var = 1, num.text = 2,
                  
                  introduction = "A study on Territorial food systems is done and we want to find the topics discussed in the description of each Territorial food system, we use topic modelling and latent dirichlet allocation. Voici les mots qui ont les indices beta (probabilit√© d'appartenance du mot dans le topic) les plus √©lev√©s pour chaque topic.",
                  request = "We want to automatically put a name on those topics based on the words caracterising them. Please give a name to each topic suming up most of the words caracterising it. Do it in French. Only give la Liste des Noms de Groupe Attribu√©s",, 
                  isolate.groups = F, drop.negative = T, generate = F)
)
)
```

Avec beta et frex  :
```{r, eval = F}

# avec les deux 
# betafrex <- data.frame(rbind(frex.df.pivot, beta.df.pivot))
# 
# res.both <- (gemini_generate(nail_textual(dataset = betafrex, num.var = 1, num.text = 2,
#                   
#                   introduction = "A study on Territorial food systems is done and we want to find the topics discussed in the description of each Territorial food system, we use topic modelling and latent dirichlet allocation. Tu as les 7  mots qui ont les indices beta (probabilit√© d'appartenance du mot dans le topic) les plus √©lev√©s pour chaque topic., et les 7 mots qui ont les indices frex (fr√©quence exclusivit√©) les plus √©lev√©s pour chaque topic.",
#                   request = "We want to automatically put a name on those topics based on the words caracterising them. Please give a name to each topic suming up most of the words caracterising it. Do it in French. Only give la Liste des Noms de Groupe Attribu√©s. Prends en compte les deux indicateurs mais ne sur-interpr√®te pas les th√®mes, tous les mots de doivent pas obligatoirement √™tre utilis√©s. ",, 
#                   isolate.groups = F, drop.negative = T, generate = F)
# )
# )
# 
# cat(res.both)

```


Comme nous l'avons d√©velopp√©, certains termes sont trop fr√©quents dans tous les topics et 'polluent' donc l'analyse textuelle et l'identification des nos th√®mes. 

*Th√®mes Communs et Transversaux :**
 **Agriculture et Alimentation :** Presque tous les topics (`1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`, `9`) incluent des termes tels que "agricole", "agriculture", "alimentaire" ou "alimentation", confirmant que le c≈ìur de l'√©tude concerne la production et la consommation de nourriture.
*   **Local et Territorial :** La forte pr√©sence des mots "local" et "territorial" (`1`, `2`, `3`, `5`, `6`, `7`, `8`, `9`) souligne l'ancrage g√©ographique et l'√©chelle d'analyse de ces syst√®mes.


# M√©thodologie

On va ensuite essayer de cr√©er des "formes fortes" , en r√©alisant de nombreuses
fois la LDA, puis en supprimant les mots les moins fr√©quents (n'apparaissant par
exemple que dans un seul topic d'une seule LDA), et en regardant comment les
m√™mes mots s'associent de la m√™me fa√ßon ensemble avec plusieurs it√©rations de
l'algorithme. Les mots ne sont pas mis dans le m√™me topic √† chaque LDA, donc le
'topic 1' de la 'lda1' n'est pas le m√™me que le 'topic1' de la 'lda2' mais s'il
y a une stabilit√© dans les th√®mes alors les m√™mes mots se retrouveront dans les
m√™mes topics, et l'on s'int√©resse justement aux termes qui composent ces topics
(= la variable latente √† nommer) plut√¥t qu'aux topics (1, 2, ... etc) en
eux-m√™mes.

Nous avons test√© plusieurs K, (9, 10, 15) et nous avons d√©cid√© de conserver k =
9 (augmenter le nombre de topics a eut pour effet que les mots associ√©s dans les
topics n'est pas forc√©ment de sens ensemble donc la construction de l'espace
latent des topics et la classification √©taient peu satisfaisantes).

Nous avons essay√© de r√©aliser la proc√©dure en gardant les termes ayant les frex
et les scores beta les plus √©lev√©s, et apr√®s avoir test√© avec uniquement les
scores frex, les groupes sont beaucoup plus discrimin√©s donc nous avons d√©cid√©
de ne conserver que cet indicateur permettant de construire des topics avec des
termes suffisamment exclusifs de chaque topic.

Nous avons aussi essay√© de conserver tous les mots dans l'espace latent final,
cependant appliquer un filtre de fr√©quence (si un terme n'apparait qu'un nombre
x minimal de fois dans l'ensemble des ex√©cutions de l'algorithme) permet de
r√©duire le nombre de points et de conserver des mots qui se retrouvent dans au
moins plusieurs lda.

Nous r√©alisons n = 10 LDA que nous lan√ßons √† partir d'une seed al√©atoire (que
ici nous fixerons pour que les r√©sultats soient exactement identiques).

# R√©alisation

## Initialisation al√©atoire et hyperparam√®tres

```{r}
seeds = sample(1:9999, 10, replace = F)

seeds = c(3644,491,1509,3734,4753,9597,7323,9369,659,4999) # vecteur conserv√© pour la reproductibilit√©

k = 9  # nb de topics 
nb_lda = length(seeds) # nb de lda 
```

## Cr√©ation des mod√®les

```{r}
# model1 <- lda.model(k = k, seed = seeds[1])
# model2 <- lda.model(k = k, seed = seeds[2])
# model3 <- lda.model(k = k, seed = seeds[3])
# model4 <- lda.model(k = k, seed = seeds[4])
# model5 <- lda.model(k = k, seed = seeds[5])
# model6 <- lda.model(k = k, seed = seeds[6])
# model7 <- lda.model(k = k, seed = seeds[7])
# model8 <- lda.model(k = k, seed = seeds[8])
# model9 <- lda.model(k = k, seed = seeds[9])
# model10 <- lda.model(k = k, seed = seeds[10])

# save(model1,model2,model3,model4,model5,model6,model7,model8,model9,model10, file = "data/modeles.RData")
load("data/modeles.RData")

```

## Extraction des termes

```{r}
# words1  <- topic.extraction(model1)
# words2  <- topic.extraction(model2)
# words3  <- topic.extraction(model3)
# words4  <- topic.extraction(model4)
# words5  <- topic.extraction(model5)
# words6  <- topic.extraction(model6)
# words7  <- topic.extraction(model7)
# words8  <- topic.extraction(model8)
# words9  <- topic.extraction(model9)
# words10 <- topic.extraction(model10)

# head(words1)
# 
# rm(model1)
# rm(model2)
# rm(model3)
# rm(model4)
# rm(model5)
# rm(model6)
# rm(model7)
# rm(model8)
# rm(model9)
# rm(model10)

# ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏èavec la nouvelle fonctipn topic.extraction2 üê≤ : 
modeles = list(model1,model2,model3,model4,model5,model6,model7,model8,model9,model10)
lda_list = topic.extraction2(modeles, nb.frex = 7)
```

on va ensuite mettre tous les mots ensemble :

```{r}
# lda_list <- list(words1,words2,words3,words4,words5,words6,words7,words8,words9,words10)
# on cr√©√© la liste avec les mots de chaque lda : n objets, contenant chacun k √©l√©ments (9 ici)

words <- unique(unlist(lda_list))# r√©cup√©re la liste unique des mots de toutes les lda
# ajouter toutes les listes de mots 

# rm(words1)
# rm(words2)
# rm(words3)
# rm(words4)
# rm(words5)
# rm(words6)
# rm(words7)
# rm(words8)
# rm(words9)
# rm(words10)

save(lda_list, file = "data/lda_list.RData")
```

et on cr√©√© un tableau de donn√©es contenant en ligne les mots, en colonne chaque
topic de chaque lda, et au croisement un "1" si le mot se retrouve dans les
termes s√©lectionn√©s caract√©risant ce topic, et sinon un "0".

```{r}
# Cr√©er le data frame 

col = paste0("lda", rep(1:nb_lda, each = k), "_topic", rep(1:k, times = nb_lda))
# on cr√©e un vecteur avec les noms de colonnes avec x lda et n topics, et on le met dans l'ordre des lda
# c'est le vecteur des noms de colonnes de notre df 

mfa.df = data.frame(matrix(ncol = k *nb_lda, nrow = length(words), NA))
# on cr√©√© le df => 1 + k * n colonnes , on le remplit de NA
colnames(mfa.df) <- col
# on met les noms de colonnes issus du vecteur col
rownames(mfa.df) <- words 
# on ajoute les mots dans la colonne words
```

On va ensuite remplir le df :

```{r}
# On va ensuite associer √† chaque lda les mots des diff√©rents topics 

for (i in 1:nb_lda) {
  for (j in 1:k) {
    col_name <- paste0("lda", i, "_topic", j)
    
    # r√©cup√©ration des mots du topic j de la lda i
    words_in_topic <- lda_list[[i]][[j]]
    # lda_list[[i]][[j]] retourne les mots du topic j de la lda i
    
    # pour chaque mot du dataframe
    mfa.df[[col_name]] <- ifelse(
      rownames(mfa.df) %in% words_in_topic,
      1,  # si le mot est dans ce topic
      0  
    )
  }
}

head(mfa.df[,1:10])
```

Et on impose un filtre qui va d'abord calculer la somme de chaque ligne = le
nombre de 1 = le nombre d'occurence de chaque mot sur l'ensemble des lda dans
l'ensemble des topics Si cette fr√©quence est de 1, alors le mot (la ligne) est
supprim√©e. On r√©cup√®re ensuite le df final.

```{r}
mfa.df.filtre <- mfa.df
mfa.df.filtre$freq <- apply(mfa.df, 1, sum)

# avec freq >= 2
mfa.df.filtre <- subset(mfa.df.filtre, freq>=2)

mfa.df.final <- mfa.df.filtre[,-ncol(mfa.df.filtre)] # supprimer la colonne fr√©quence 

summary(colSums(mfa.df.final)) # on v√©rifie le nombre min, max, m√©dian, moyen de mots par th√®mes pour voir l'impact du filtrage 

dim(mfa.df)
dim(mfa.df.final)

# save(mfa.df.final, file = "data/mfa.df.final.RData")
```

On va ensuite comparer deux m√©thodes pour l'obtention des formes fortes : un AFM
avec autant de groupes de variables que de nb.lda et pour chaque groupe autant
de variables que de k, et une AFC avec une transformation des fr√©quences en
modalit√©s, et autant de variables de termes uniques dans le vocabulaire.

## AFM

L'AFM est une analyse dimensionnelle qui permet d'imposer une structure sur les
variables, ce qui est n√©cessaire ici car chaque topic n'a de sens qu'au sein
d'une ex√©cution de la LDA, et les mots peuvent √™tre parfois partag√©s au sein
d'une lda ou entre topics de plusieurs LDA.

```{r}
res.mfa <- MFA(base = mfa.df.final, 
              group = rep(k,nb_lda),
              type = rep("f",nb_lda), 
              name.group = paste0("lda",seq(1:nb_lda)),
              graph = FALSE
              )

# on affiche le scree plot pour choisir le nombre de composantes √† conserver

barplot(res.mfa$eig[,2])
# on retient 5 composantes

res.mfa <- MFA(base = mfa.df.final, 
              group = rep(k,nb_lda),
              type = rep("f",nb_lda), 
              name.group = paste0("lda",seq(1:nb_lda)),
              ncp = 5
              # graph = F
              )

plot.MFA(res.mfa, choix = "freq", invisible = "ind")

coord_topics <- as.matrix(res.mfa$freq$coord)
```


### Fortification des topics 

On r√©alise ensuite une classification hierarchique de nos individus (mots).

```{r}
clustopt <- HCPC(res = res.mfa, nb.clust = -1) # nb optimal de classes
# 6 = nb optimal de classes
```

On regarde le nombre optimal de clusters, les mots qui les composent et on
essaye d'optimiser ce nombre de clusters.

```{r}
clust7<- HCPC(res = res.mfa, nb.clust = 7, graph = FALSE)
clust8<- HCPC(res = res.mfa, nb.clust = 8, graph = F)
```

Quel nombre de cluster a le plus de sens ?

```{r, fig.width=8, fig.height=5}
plot_clust <- function(res.hcpc){
  
  clust <- data.frame(cbind(word = rownames(mfa.df.final),
                           clust = res.hcpc$data.clust$clust, 
                           dim1 = res.mfa$ind$coord[,1], 
                           dim2 = res.mfa$ind$coord[,2]))

  str(clust)
  clust$clust <- as.factor(clust$clust)
  clust$word <- as.factor((clust$word))
  clust$dim1 <- as.numeric(clust$dim1)
  clust$dim2 <- as.numeric(clust$dim2)
  
  plot <- plot_ly(
  data = clust,
  x = ~dim1,
  y = ~dim2,
  type = "scatter",
  mode = "markers+text",
  color = ~factor(clust),
  text = ~word,       # nom de ta colonne contenant les mots
  textposition = "top center",
  marker = list(size = 7),
  hoverinfo = "text"
) %>%
  layout(
    title = "Projection des mots dans l'espace factoriel (Dim 1 & Dim 2)",
    xaxis = list(
      title = "Dimension 1",       # supprime le titre
      showticklabels = FALSE,  # supprime les graduations
      zeroline = FALSE
    ),
    yaxis = list(
      title = "Dimension 2",
      showticklabels = FALSE,
      zeroline = FALSE
    ),
    legend = list(title = list(text = "Cluster"))
  )
  
  plot
}
```

```{r}
# nombre optimal 
plot_clust(clustopt)
plot_clust(clust7)
plot_clust(clust8)
```

On choisit le nombre de clusters retenus :

```{r}
res.hcpc <- HCPC(res = res.mfa, nb.clust = 6)
save(res.hcpc, file = "data/res.hcpc.RData")
```

et on extrait les coordonn√©es des individus et leur appartenance √† un cluster :

```{r}
# cr√©ation du jdd contenant les mots, leurs coordonn√©es sur les dimensions 1 et 2 ainsi que leur topic
clust <- data.frame(cbind(word = rownames(mfa.df.final),
                           clust = res.hcpc$data.clust$clust, 
                           dim1 = res.mfa$ind$coord[,1], 
                           dim2 = res.mfa$ind$coord[,2]))

str(clust)
clust$clust <- as.factor(clust$clust)
clust$word <- as.factor((clust$word))
clust$dim1 <- as.numeric(clust$dim1)
clust$dim2 <- as.numeric(clust$dim2)

head(clust)

save(clust, file = "data/clust.RData")
```

On r√©cup√®re ensuite l'ensemble des mots de chaque clusters et on r√©alise
l'extraction de la variable latente (nom du topic)

```{r, eval = F}

# r√©cup√©ration des mots de chaque clusters 
mots.clust <- tapply(
  clust$word,     # Le vecteur √† appliquer la fonction (les Mots)
  clust$clust, # Le facteur de regroupement (les Clusters)
  paste,                # La fonction √† appliquer
  collapse = ", ")       # L

# interpr√©tation avec naileR (√† faire plus tard)

cat(gemini_generate(nail_textual(dataset = clust[,1:2], num.var = 2, num.text = 1,

                  introduction = "A study on Territorial food systems is done and we want to find the topics discussed in the description of each Territorial food system, we use topic modelling and latent dirichlet allocation. We consolidate our topics by using multiple executions of the algorithm and perform MFA to find strong forms in our topics.",
                  request = "We want to automatically put a name on those topics based on the words caracterising them. Please give a name to each topic suming up most of the words caracterising it. Do it in French. Only give la Liste des Noms de Groupe Attribu√©s. Is there a topic who is closer than other for talking about social justice, inequality those kind of things ?",,
                  isolate.groups = F, drop.negative = T, generate = F)
                  )
    )

#res

descr.topic <- gemini_generate(res)

cat(descr.topic)

```


Comparison of All Groups

Les six groupes de mots, identifi√©s comme des th√®mes distincts au sein des syst√®mes alimentaires territoriaux (SAT), r√©v√®lent des facettes compl√©mentaires de ces dynamiques complexes:

*   **Planification et Animation Territoriale (Groupe 1)**: Ce th√®me met l'accent sur les processus de gouvernance, de collaboration et de mise en ≈ìuvre strat√©gique des projets alimentaires territoriaux. Il souligne l'importance des partenariats, de l'engagement communautaire et des cadres d'action pour structurer le SAT.

*   **Restauration Collective et √âducation Alimentaire (Groupe 2)**: Ce groupe se concentre sur les enjeux de la demande et de la consommation, en particulier au sein de la restauration collective (notamment scolaire). Il aborde les initiatives visant √† promouvoir une alimentation saine, durable et locale, la lutte contre le gaspillage, et l'√©ducation des consommateurs, souvent en lien avec des politiques publiques comme EGalim.

*   **Typologies Territoriales et Productions Sp√©cifiques (Groupe 3)**: Ce th√®me d√©crit les caract√©ristiques g√©ographiques, d√©mographiques et productives des territoires. Il distingue diff√©rents types d'espaces (urbains, ruraux, montagneux, p√©riurbains) et les productions agricoles ou transform√©es sp√©cifiques qui y sont associ√©es, offrant un aper√ßu du contexte intrins√®que de chaque SAT.

*   **Enjeux Climatiques, Environnementaux et Sant√© (Groupe 4)**: Ce groupe met en lumi√®re les d√©fis majeurs auxquels les SAT sont confront√©s. Il couvre les impacts du changement climatique (s√©cheresse, gestion de l'eau), la pr√©servation de la biodiversit√©, ainsi que les questions de sant√© publique li√©es √† l'alimentation (ob√©sit√©, maladies), soulignant la n√©cessit√© d'adaptation et de r√©silience.

*   **Analyse Socio-√âconomique et Agricole (Groupe 5)**: Ce th√®me est ax√© sur les indicateurs quantitatifs et les dynamiques √©conomiques et sociales. Il analyse les donn√©es statistiques concernant la production agricole (cheptel, c√©r√©ales, SAU), les conditions socio-√©conomiques des m√©nages (pauvret√©) et les tendances g√©n√©rales √† diff√©rentes √©chelles territoriales.

*   **Logistique et Commercialisation des Fili√®res (Groupe 6)**: Ce groupe aborde les aspects pratiques de la cha√Æne d'approvisionnement et de la mise sur le march√© des produits. Il traite des d√©fis li√©s √† la distribution, √† la commercialisation, √† la logistique et aux d√©bouch√©s pour les producteurs, notamment pour les fruits et l√©gumes, identifiant les freins et les leviers d'action.

En somme, ces th√®mes dessinent un panorama holistique des syst√®mes alimentaires territoriaux, couvrant leur gouvernance, leurs interactions avec les consommateurs, leurs sp√©cificit√©s g√©ographiques, leurs vuln√©rabilit√©s environnementales, leurs fondements socio-√©conomiques et leurs m√©canismes de march√©.

Liste des Noms de Groupe Attribu√©s

*   **Groupe 1**: Planification et Animation Territoriale
*   **Groupe 2**: Restauration Collective et √âducation Alimentaire
*   **Groupe 3**: Typologies Territoriales et Productions Sp√©cifiques
*   **Groupe 4**: Enjeux Climatiques, Environnementaux et Sant√©
*   **Groupe 5**: Analyse Socio-√âconomique et Agricole
*   **Groupe 6**: Logistique et Commercialisation des Fili√®res

### Classification des ldas des topics en 6 clusters par Kmeans 

```{r}
clusters_topics <- kmeans(x = coord_topics, centers = 6)

clusters_topics_df <- data.frame(cbind(coord_topics,clust = clusters_topics$cluster))
clusters_topics_df$clust <- as.factor(clusters_topics_df$clust)

# save(clusters_topics_df, file = "data/clusters_topics_df.RData")
# save(res.mfa, file = "data/res.mfa.RData")
load(file = "data/clusters_topics_df.RData")

# On va renommer les noms des clusters de 1 √† 6 avec le nom du clusters
# On va ensuite extraire quels topics de quels lda ont permis de construire ce cluster 

clusters_topics_df <- clusters_topics_df %>%
  mutate(clust = fct_recode(clust,
    secteur_agri        = "4",
    territoires         = "5",
    environnement_sante       = "2",
    economie_alimentaire= "1",
    gouvernance         = "6",
    education           = "3"
  ))

clusters_topics_df %>%  
  ggplot() +
  aes(x= Dim.1, y = Dim.2, col = clust) +
  geom_point() +
  stat_ellipse() +
  scale_color_manual(values = c("chartreuse2","saddlebrown","springgreen4", "gold1","royalblue","tomato2")) 
```

On va reprendre le dataframe avec les theta, on va tous les combiner avec l'ensemble des topics de l'ensemble des LDA. On va renommer les colonnes, le nom du topic de la lda 

### Obtention des vecteurs de composition FORTIFIES 
```{r}
# theta_all contient 90 colonnes : 9 topics * 10 LDA 
load("data/modeles.RData")

theta_all <- data.frame(cbind(model1$theta, model2$theta, model3$theta, model4$theta, model5$theta, 
                              model6$theta, model7$theta, model8$theta, model9$theta , model10$theta))

rm(model1)
rm(model2)
rm(model3)
rm(model4)
rm(model5)
rm(model6)
rm(model7)
rm(model8)
rm(model9)
rm(model10)

colnames(theta_all) <- colnames(mfa.df.final)

theta_all <- round((theta_all*100),2)

head(theta_all)

# save(theta_all, file = "data/theta_all.RData")
```


```{r}
# on va renommer ensuite les colonnes avec le nom des clusters, et aggr√©ger ensuite par cluster dans "theta_resume" 
theta_resume <- theta_all

# on renomme les noms des colonnes de theta_resume 
colnames(theta_resume) <- clusters_topics_df$clust    # => correspondance entre le nom des clusters et chaque topic de chaque lda 

head(theta_resume)
```


```{r}
# et on calcule les nouvelles fr√©quences sur les 6 th√©matiques fortifi√©es 
theta_resume_unique <- as.data.frame(
  t(rowsum(
      t(theta_resume),
      group = colnames(theta_resume)
    )))

theta_resume_unique <- 100*theta_resume_unique / 1000 # on passe en %

# save(theta_resume_unique, file = "data/theta_resume_unique.RData")
load(file="data/theta_resume_unique.RData")

head(theta_resume_unique)
```



### Vecteur de composition en se basant sur les formes fortes

Une autre approche pour obtenir les compositions des PAT avec les formes fortes et d'assigner chaque topic de lda √† une forme forte en calculant la distance avec les centroides et d'assigner chaque topic de lda √† la forme forte avec laquelle il a la plus petite distance.

```{r}
#Determination de l'association entre forme forte et topics de lda

#On recup√®re les coordonn√©es de nos topics de LDA sur l'AFM que nous avons fait
freq <- as.data.frame(res.mfa$freq$coord)
freq$clust <- NA

#On r√©cup√®re les coordonn√©es des PAT et on rajoute le cluster auquel ils sont associ√©s pour calculer les centroides
PAT_coord <- res.mfa$ind$coord
PAT_coord <- as.data.frame(cbind(PAT_coord,res.hcpc$data.clust$clust))
colnames(PAT_coord)[6] <- c("clust")

#On calcule les centroides
centre <- PAT_coord %>% 
  group_by(clust) %>% 
  summarise(across(c(Dim.1,Dim.2,Dim.3,Dim.4,Dim.5), mean))

#On r√©ordonne notre tableau pour avoir les 5 dimensions et le cluster pour ensuite calculer la distance
centre <- centre[,c(2:6,1)]

#On fusionne les 2 tableaux, on calcule les distances entre toutes nos lignes
combined <- rbind(freq[,-6], centre[,-6])
distance <- as.matrix(dist(combined))

nA <- nrow(freq)
nB <- nrow(centre)

distance_centre_topic <- distance[1:nA, (nA + 1):(nA + nB)]
colnames(distance_centre_topic) <- c("Gouvernance","Education alim","Territoires","Environnement","Secteur agricole","Economie alimentaire")

class <- apply(distance_centre_topic,1, which.min)
freq$clust <- as.factor(class)

freq <- freq %>%
  mutate(clust = fct_recode(clust,
                            gouvernance        = "1",
                            education_alim         = "2",
                            territoires       = "3",
                            environnement= "4",
                            secteur_agri         = "5",
                            economie_alimentaire           = "6"
  ))

library(tidyverse)

freq %>% 
  ggplot(aes(x=Dim.1,y=Dim.2, col = clust)) +
  geom_point() +
  stat_ellipse()+
  theme_bw()

theta_resume2 <- theta_all
colnames(theta_resume2) <- freq$clust
theta_resume2 <- t(rowsum(t(as.matrix(theta_resume2)), group = colnames(theta_resume2), na.rm = T))
theta_resume2 <- theta_resume2/10

# save(theta_resume2, file = "data/theta_resume2.RData")

colnames(theta_resume2) <- paste0(colnames(theta_resume_unique),"_2")

diff <- theta_resume_unique-theta_resume2

diff$sum_diff <- apply(X = diff, 1, function(x) sum(abs(x)) )  

```

On pr√©pare notre premier tableau de composition (fr√©quence sur le comptage simple des termes forts) de nos PAT en forme forte

```{r}
load("data/df_resultat.final.RData")
mfa_0 <- t(df_resultat.final)
rownames(mfa_0) <- gsub(x=rownames(mfa_0),"text","")

mfa_0 <- mfa_0 / rowSums(mfa_0)

colnames(mfa_0) <- c("Gouvernance_3","Education_alim_3","Territoire_3","Environnement_3","Secteur_agri_3","Economie_alimentaire_3")
colnames(theta_resume2) <- paste0(colnames(theta_resume_unique),"_2")

mfa_0 <- as.data.frame(mfa_0)
mfa_0 <- mfa_0[,c(6,2,4,1,5,3)]
mfa_0 <- mfa_0 * 100
mfa_0$num <- as.numeric(rownames(mfa_0))

```

Pour r√©aliser la typologie √† partir de ces vecteurs de proportions en 6 topics fortifi√©s, on va utiliser ces deux approches m√©thodologiques et r√©aliser une AFM 
```{r}
theta_df_combine <- data.frame(cbind(theta_resume_unique, theta_resume2))
theta_df_combine$num <- sort(mfa_0$num)

theta_df_combine <- theta_df_combine %>%left_join(mfa_0)
theta_df_combine <- theta_df_combine[,-13]

res.mfa.combine <- FactoMineR::MFA(base = theta_df_combine, 
                                   group = c(6,6,6),
                                   type = c("f","f","f"))

# save(file=res.mfa.combine,"data/res.mfa.combine.RData")

plot.MFA(x = res.mfa.combine, choix = "freq", invisible = "ind")
```



### Typologie de PAT sur vecteurs fortifi√©s 

```{r}
# On va faire la classification sur l'un de ces tableaux de donn√©es
# ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è  choix du nombre de dimensions conserv√©es !!!!!!ne n donnera pas le m√™me nombre de clusters √† la CAH 
res.afc.theta <- FactoMineR::CA(theta_resume2, ncp = 5)
barplot(res.afc.theta$eig[,2])

res.hcpc.combine <- HCPC(res.afc.theta,nb.clust=3)

res.hcpc.combine$desc.var


```

Nos clusters sont totalement coh√©rents avec les clusters pr√©c√©demment form√©s avec la m√©thode "basique". 
Ils surrepr√©sentent les m√™mes th√©matiques et en opposition sous-repr√©sentent √©galement les m√™mes th√©matiques.

On va faire la classification avec l'autre m√©thode utilisant les centroides. 

```{r}
# On va faire la classification sur l'un de ces tableaux de donn√©es
res.afc.theta.2 <- FactoMineR::CA(theta_resume2, ncp = 3)
barplot(res.afc.theta.2$eig[,2])

res.hcpc.combine2 <- HCPC(res.afc.theta.2,nb.clust=-1)

res.hcpc.combine2$desc.var


res.afc.theta.2 <- FactoMineR::CA(mfa_0[,-7])
barplot(res.afc.theta.2$eig[,2])

res.hcpc.combine2 <- HCPC(res.afc.theta.2,nb.clust=-1)
res.hcpc.combine2$desc.var

```


Illustration des clusters  : 
```{r}
# on va combiner theta_resume_unique avec df_clust_ill 

load("data/df_textes.RData") #‚ö†Ô∏è ON CHANGE LES ROWNAMES AVANT LA JOINTURE SINON CE SERA PAS LES BONS PAT JOINTS 
load("data/df_clust_ill.RData")
rownames(theta_resume2) <- gsub(x = df_textes$doc, pattern = "text", replacement = "")
rm(df_textes)

theta_resume2 = as.data.frame(theta_resume2)
df_illus_freq_afc <- left_join(data.frame(cbind(num = rownames(theta_resume2), theta_resume2)),
                                data.frame(cbind(num = rownames(df_clust_ill), df_clust_ill)) , by = 'num')

df_illus_freq_afc <- df_illus_freq_afc[,-1]


# on refait ensuite l'AFC avec des variables illustratives suppl√©mentaires

# on doit supp certaines colonnes car valeurs manquantes dans variables quanti 
# ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

df_illus_freq_afc <- df_illus_freq_afc %>%
  select(
    -taux_de_pauvrete_du_territoire,
    -emission_de_ges,
    -nombre_de_captage_prioritaire_grenelle_dans_le_perimetre,
    -evolution_de_lirrigation_depuis_5_ans
  )

# quali et quanti additionnel (voir bilan fin dec )
quanti.sup <- which(sapply(df_illus_freq_afc, is.numeric))
quanti.sup <- as.numeric(quanti.sup[-c(1:6)]) # on retire les colonnes 1 √† 6 qui seront les colonnes actives
quali.sup  <- as.numeric(which(sapply(df_illus_freq_afc, function(x) is.factor(x) || is.character(x))))


test_illus <- df_illus_freq_afc[,-quali.sup]

res.afc.theta.2.illus <- FactoMineR::CA(test_illus,
                                        quanti.sup = 7:47, ncp = 3

                                          ) 

# NOMBRE DE DIMENSIONS DE LA CA ? 

res.hcpc.test <- HCPC(res.afc.theta.2.illus, nb.clust = -1)
res.hcpc.test$desc.var$frequency
```


Graphique des clusters am√©lior√© : 

```{r}
##############################
# cr√©ation du dataframe
# ‚ö†Ô∏è etre sur de bien join sur les bons individus !!! ‚ö†Ô∏è

df_hcpc <- data.frame(clust = res.hcpc.test$data.clust$clust)
rownames(df_hcpc) = rownames(res.hcpc.test$data.clust)
df_hcpc <- df_hcpc %>% arrange(as.numeric(rownames(df_hcpc))) # on retrie par nom de lignes

# et on combine avec les coordonn√©es sur les axes 1 et 2
df_hcpc <- data.frame(cbind(df_hcpc, res.afc.theta.2.illus$row$coord[,1:2]))

# on passe clust en factor 
df_hcpc$clust = as.factor(df_hcpc$clust)


##########################
# graphique
df_hcpc %>% ggplot() +
  aes(x = Dim.1, y = Dim.2, col = clust) +
  geom_point(size = 1.5) +
  
  theme_bw() +
  
  ggtitle ("Typologie des PAT construite sur leur proportion en topics fortifi√©s dans leur description", 
           subtitle = "349 PAT sont utilis√©s pour construire cette typologie") +
  
  theme(plot.title = element_text(size = 13, face= "bold", hjust = 0.5), 
        plot.subtitle = element_text(size = 10), 
        axis.title.x = element_text(size = 12), 
        axis.title.y = element_text(size = 12), 
        legend.title =  element_text(size = 12),
        legend.text =  element_text(size = 10 )
        
        ) +
  
  scale_color_manual(
    # nom des groupes 
    label = c("Groupe1", "Groupe2", "Groupe3"), 
    values = c("royalblue", "limegreen", "darkorange"))+ 
  
  labs(
      x = "Dim 1",
      y = "Dim 2",
      color = "Cluster"
    ) +

  coord_equal(xlim = c(-1.50, 1.50), ylim = c(-1.50,1.50))

  
```



## Test AFM quali

Nous avons aussi essay√© une AFM avec comme classe de variable 'n' =\>
cat√©gorielle.

```{r, eval = F}
afm.quali <- data.frame(lapply(mfa.df.final, FUN = as.factor ))
rownames(afm.quali) <- rownames(mfa.df.final)
# str(head(afm.quali))

res.fma.q <- MFA(base = afm.quali, 
                 group = rep(k,nb_lda),
                 type = rep("n",nb_lda), 
                 name.group = paste0("lda",seq(1:nb_lda))
                 )
```

Il semblerait difficile d'identifier des formes fortes car en utilisant un type
cat√©gorielle, les mots '...' , ... tirent fortement les axes donc cette m√©thode
ne permet pas de consolider notre LDA.

## AFC

```{r, eval = F}
afc.df <- data.frame(t(mfa.df.final))
# str(head(afc.df))

res.afc <- CA(afc.df)
barplot(res.afc$eig[,2]) # on retient 5 ncp 

res.afc <- CA(afc.df, ncp = 5)

plot.CA(res.afc, invisible = "row")

```

Si on fait une classification issue du r√©sultat de l'AFC, il semblerait que les
formes fortes observ√©es soient les m√™mes que dans la m√©thode 1.

```{r, eval = F}
# coord.afc <- data.frame(cbind(dim1= res.afc$col$coord[,1], 
#                               dim2= res.afc$col$coord[,1]))
# head(coord.afc)
# 
# hcpc.afc <- HCPC(coord.afc, nb.clust = -1)
# 
# plot_clust(hcpc.afc)
# 
```

# Conclusion

Il semble int√©ressant d'utiliser des analyses factorielles pour consolider nos
topic modellings et obtenir des formes fortes de nos topics. Apr√®s avoir lanc√©
cet algorithme en faisant varier beaucoup les param√®tres, on a pu observer une
relative sensabilit√© √† la modification de la stop_words liste des mots, ainsi
√©videmment qu'au param√®tre k, ainsi que des topics tr√®s "forts" toujours
retrouv√©s et compos√©s des m√™mes termes.

Le r√©sultat pr√©sent√© (6 clusters) nous semble coh√©rents au vue de tous les
essais r√©alis√©s, rendant compte de th√©matiques qui portent du sens (bien que
certaines soient compos√©s de plusieurs sous-sujets), et que l'on a retrouv√©
syst√©matiquement.
