---
title: "Bilan analyse textuel - LDA - Projet PAT"
author: "Erwan Gouhier, Anna Mathieu"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    toc_float: true
    self_contained: true
    df_print: paged
    highlight: tango
    tabset: true
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r packages, include=F}
library(topicmodels)
library(tidytext)
library(stringr)
library(quanteda)
library(udpipe)
library(tidyverse)
library(dplyr)
library(remotes)
library(readxl)


# Ces 2 packages sont obtenus en les téléchargeant depuis GitHub

# remotes::install_github("lvaudor/mixr")
library(mixr)
# remotes::install_github("trinker/lemmar")
library(lemmar)
```

# Introduction

Nous avons voulu réaliser une LDA (Latent Dirichlet Analysis) pour avoir un
aperçu des thèmes abordés par les descriptions de nos PAT. Un travail de
pré-traitement des données à donc été réalisé car les données ne sont pas
utilisables en l'état.

On trouve par exemple : des débuts et des fins de phrases fusionnées (ex: "Zoom
sur les filières déficitairesDans le" - PAT du département de la Savoie), des
mots mal-ortographiés...etc

Un travail de lemmatisation en plusieurs temps a également été réalisé afin
d'obtenir quelque chose de plus facilement interprétable.

# Importation des données

```{r}
pat2025 <- read.csv("data/pats-20250710-win1252.csv", header = T, sep = ";", fileEncoding = "CP1252", dec = ".") # CP1252 permet de gérer les apostrophes non détectées 

pat2025[pat2025== ""] <- NA # Transformation des cases vides en NA
```

# Filtrage des données

Nous avons réalisé plusieurs filtres pour nettoyer nos données.

## Changement des apostrophes

Il existe plusieurs types d'apostrophes, nous nous sommes rendus compte que
l'apostrophe que nous pouvions réaliser avec nos claviers d'ordinateurs
\<\<'\>\> sont différents de ceux présents dans nos descriptions \<\<’\>\>, ils
ne sont donc pas reconnus si on veut les filtrer, pour des facilités d'écriture
nous les avons remplacé.

```{r}
pat2025$descriptions_libre_des_enjeux_du_territoire_par_le_porteur <- gsub("’", "'", pat2025$descriptions_libre_des_enjeux_du_territoire_par_le_porteur, fixed = TRUE)
```

## Liens vers des sites internet

Un autre filtre que nous avons mis en place consiste en la suppression des
chaines de caractères continues qui commencent par "https" et "www" qui
contiennent par la suite des caractères dans un ordre aléatoire qui parasite nos
tokens si on ne les enlève pas.

```{r}
#On retire les URL
pat2025$descriptions_libre_des_enjeux_du_territoire_par_le_porteur <- gsub("https?://\\S+", "", pat2025$descriptions_libre_des_enjeux_du_territoire_par_le_porteur)
pat2025$descriptions_libre_des_enjeux_du_territoire_par_le_porteur <- gsub("www.\\S+", "", pat2025$descriptions_libre_des_enjeux_du_territoire_par_le_porteur)

# df pour desc 
textdata <- as.data.frame(cbind(doc_id = pat2025$nom_administratif, text = pat2025$descriptions_libre_des_enjeux_du_territoire_par_le_porteur))

save(pat2025, file = "data/pat2025.RData")
```

## Saut de lignes

Comme dit dans l'introduction certains textes ont des problèmes de retour à la
ligne ce qui revient à fusionner des mots. On suppose que c'est dû à la mise en
forme des descriptions dans les cellules d'Excel. On réduit ensuite les espaces
multiples en 1 seul espace grâce à la fonction str_squish.

```{r}
# gérer le pb des sauts de lignes non pris en compte
textdata$text <- gsub("(?<=[a-z])(?=[A-Z])", " ", x =  textdata$text, perl = TRUE) # décole les MAJ collées à des minuscules (précédées par des minuscules)

# supp db espace doubles ou +
textdata$text <- str_squish(textdata$text)
txt <- textdata$text
```

## Noms des communes

Nous avons ensuite décidé d'enlever le nom des communes. On considère qu'elle ne
porte pas d'information direct sur les PAT et on préfère donc les enlever. Pour
filtrer le nom des communes, nous avons extrait les noms à partir de la colonne
communes_nom.

Comme certaines villes sont également référencées par des parties de leur nom
comme Saint-Nazaire parfois écrit "St Nazaire", on extrait tout les noms en
majuscules des noms de communes. Le fait de laisser les mots en majuscules nous
permet d'éviter de supprimer des mots communs. Un exemple illustratif dans un
cas général pourrait être une ville comme Collonges-la-Rouge, avec notre méthode
si Collonges-la-Rouge est écrit sans les tirets on supprimera le mot "Rouge"
mais on gardera le mot "rouge".

Pour appliquer ces filtres on a besoin de la fonction escape_regex qui permet
d'utiliser str_replace_all avec des chaines de caractères qui contiennent des
caractères spéciaux. On applique également un filtre strict avec le notation
"\\\\b", ainsi le mot "Port" ne retirera pas "Port" contenu dans le mot
"Portrait" en début de phrase dans la description du PAT de Redon.

```{r}
escape_regex <- function(x) {
  str_replace_all(x, "([\\.\\+\\*\\?\\^\\$\\(\\)\\[\\]\\{\\}\\|\\\\])", "\\\\\\1")
}


villes <- unlist(strsplit(pat2025$communes_nom, split = ", ")) #On récupère les noms des villes tels que donnés 

villes_maj <- gsub(pattern = "-",, replacement = " ", x = villes, fixed = TRUE) 
villes_maj <- unlist(strsplit(villes_maj, split = " "))
villes_maj <- grep("^[A-Z]", villes_maj, value = TRUE) #On récupère les mots en majuscules dans les noms des villes 

villes_filtre <- c(villes,villes_maj)

villes_filtre <- escape_regex(villes_filtre) #Pour permettre d'appliquer le filtre avec les charactères spéciaux
villes_filtre <- paste0("\\b(", paste(villes_filtre, collapse = "|"), ")\\b") #Empêcher de reconnaitres des noms de ville en début de phrase "Port" --> "Portrait"

##### Application du filtre #####

txt_clean <- str_replace_all(string=txt, villes_filtre,"")  # application du villes des filtres des villes: retire les noms de villes

txt <- tolower(txt_clean)  # passe le texte en minuscules pour la suite
```

Après tout cela on peut enfin passer nos descriptions en miniscules pour filtrer
les mots outils et procéder à la lemmatisation

## Apostrophes

On supprime les apostrophes.

```{r}
#Filtre apostrophes
txt <- str_replace_all(txt, c("l'"="","d'"="","l’" = "", "d’" = "")) # suppression des apostrophes

```

## Filtres des gentilés

```{r}
#On récupère le nom des gentilés des communes
gentile <- read.csv("data/gentile.csv",sep=";",header=TRUE)
gentile <- unlist(tolower(gentile$GENTILE))

#On crée notre filtre pour qu'il soit reconnu par str_replace_all
gentile <- paste0("\\b(", paste(gentile, collapse = "|"), ")\\b")

#On applique le filtre
txt <- str_replace_all(string=txt,gentile,"")

#On récupère le nom des gentilés des départements

gentile <- read.delim("data/gentiles_departements.txt",sep=";",header=FALSE)
gentile <- unlist(str_squish(tolower(gentile[,2])))
gentile <- paste0("\\b(", paste(gentile, collapse = "|"), ")\\b")

txt <- str_replace_all(string=txt,gentile,"")

```

## Chiffres, tirets et points

On veut supprimer les points qui vont être directement adjacent à certains mots.
Ensuite on peut observer certaines valeurs chiffrées dans les descriptions avec
des tirets entre eux ou alors directement adjacents à certains mots. On supprime
également les tirets avec un mot juste après, dans les descriptions cela
représente les énumérations de fait.

La fonction gestion_nombres fait donc dans l'ordre : - une suppression des
points - une séparation des nombres collés à des lettres - une suppression des
tirets entre des nombres et d'autres élements (nombres ou mots) - une suppresion
des tirets en début de phrase

```{r}

gestion_nombres <- function(text) {
  text <- gsub("\\.", " ", x =  text, perl = TRUE)                   # mettre des espaces à la place des points
  text <- gsub("(?<=[0-9])(?=[[\\p{L}])|(?<=[[\\p{L}])(?=[0-9])", " ", x =  text, perl = TRUE)   # mettre des espaces s'il y a un chiffre avant une lettre minuscule (ex : 2007menaces) ou si il y a un chiffre après une lettre minuscule (ex : covid19)
  text <- gsub("(?<=[0-9])-(?=[[\\p{L}])|(?<=[[\\p{L}])-(?=[0-9])|(?<=[0-9])-(?=[0-9])", " ", text, perl = TRUE) # chiffre'-'lettre ou lettre'-'chiffre ou chiffre'-'chiffre
  text <- gsub(" -", " ", x = text)                                  # supprimer les tirets en début de mots
  return(text)
}

txt <- gestion_nombres(txt)
```

## Stopwords

Pour les stops words, nous nous sommes basées sur 2 listes de stopwords que nous
avons fusionnées. La première est une liste obtenue sur le GitHub
"Stopwords-Iso" au lien suivant :
<https://github.com/stopwords-iso/stopwords-fr/tree/master>

La seconde est issu du package Xplortext avec quelques rajouts personnels que
nous avons trouvé en regardant les mots que nous obtenions après tokenisation.

Nous avons tenté de fusionner les listes, mais quand nous le faisions le filtre
ne fonctionnait pas. Nous avons donc décidé d'appliquer les 2 filtres
successivement, ce qui nous donnait un résultat satisfaisant. On applique à
nouveau un filtrage strict avec la notation "\\\\b"

```{r}
#Premier filtre

fr_stopwords <- read.table("data/stopwords-fr.txt")
# from : https://github.com/stopwords-iso/stopwords-fr/blob/master/stopwords-fr.txtas.character(chartr(                        # on retire les accents 
fr_stopwords <- as.character(fr_stopwords$V1)
filter1 <- paste0("\\b(", paste(fr_stopwords, collapse = "|"), ")\\b")
txt_filter1 <- str_replace_all(string=txt, filter1,"")
```

```{r}
#Second filtre

fr_stopwords_2 <- unlist(str_extract_all("territoire territoires plus à ai aie aient aies ait as au aura aurai auraient aurais aurait auras aurez auriez aurions aurons auront aux avaient avais avait avec avez aviez avions avons ayant ayez ayons c ce ceci cela celà ces cet cette d dans de des du elle en es est et étaient étais était étant été étée étées êtes étés étiez étions eu eue eues eûmes eurent eus eusse eussent eusses eussiez eussions eut eût eûtes eux fûmes furent fus fusse fussent fusses fussiez fussions fut fût fûtes ici il ils j je l la le les leur leurs lui m ma mais me même mes moi mon n ne nos notre nous on ont ou par pas pour qu que quel quelle quelles quels qui s sa sans se sera serai seraient serais serait seras serez seriez serions serons seront ses soi soient sois soit sommes son sont soyez soyons suis sur t ta te tes toi ton tu un une vos votre vous y plusieurs d’accord hélas peut-être donc pourtant autour derrière dessous dessus
devant parmi vers durant pendant depuis afin malgré sauf dès lorsque parce pendant pourquoi dedans loin partout aujourhui aussitôt autrefois avant-hier bientôt d'abord déjà demain en ce moment hier enfin longtemps maintenant quelquefois soudain souvent assez aussi autant davantage presque debout mieux sinon brusquement exactement doucement facilement heureusement lentement sagement seulement tranquillement st où paatfin er ème eme ha km nd aa lys hem peu fort forts très grand petit auprès basque faveur", boundary("word")))

filter2 <- paste0("\\b(", paste(fr_stopwords_2, collapse = "|"), ")\\b")
txt_filter2 <- str_replace_all(string=txt_filter1, filter2,"") 

txt_final <- str_squish(txt_filter2)
```

Une fois tous ces filtres réalisés, nous pouvons passer à la tokenisation

# Tokenisation

Pour réaliser la tokenisation, nous utilisons la fonction tokens du package
"quanteda". On lui fournit nos descriptions filtées en lui indiquant que nous
voulons que la tokenisation se face par mot. On lui ajoute les filtres
remove_punct/symbols/numbers pour enlever les potentiels symboles et chiffres
qui aurait échappé à nos précédents filtres.

```{r}
toks <- tokens(txt_final,
               what = "word",
               remove_punct = TRUE,
               remove_symbols = TRUE,
               remove_numbers = TRUE)
```

Nous mettons en suite en forme nos tokens en colonne en les associant aux
descriptions des PAT dont ils proviennent.

```{r}
tokens_df <- data.frame(
  doc = rep(names(toks), lengths(toks)),
  token = unlist(toks)
)

save(tokens_df, file = "data/tokens_df.RData" )
```

# Lemmatisation

Pour finir la préparation de notre jeu de données, nous lemmatisons nos tokens.
Pour cela nous nous basons comme pour les stopwords sur différentes bases de
lemmatisation : - hash_lemmar_fr qui provient du package lemmar - lexique382 qui
provient du package mixr - une liste obtenue sur un le GitHub de l'utilisateur
michmech : <https://github.com/michmech/lemmatization-lists/tree/master>

Nous allons associer chaque mot à sa version lemmatisé dans chacune des bases,
puis nous allons superposer les bases pour avoir un maximum de mots lémmatisés.

```{r}
# Lemmatisation par hash_lemma_fr

hash_lemma_fr$token <- tolower(hash_lemma_fr$token)
hash_lemma_fr$lemma <- tolower(hash_lemma_fr$lemma)
res.lemmat <- left_join(x=tokens_df, y = hash_lemma_fr, by = join_by(x$token==y$token))

# Performance de la première lemmatisation
length(unique(res.lemmat$token[which(is.na(res.lemmat$lemma))]))
# dim : 62063 * 2 => n'ajoute pas de doublons 

```

Dans notre code nous avons initialement réalisé nos 3 lemmatisations et regarder
les tokens pour lesquels aucune lemmatisation n'a été faite car les tokens
n'étaient présents dans aucune des bases que nous avions. Nous avons donc créer
une base de lemmatisation que nous avons fusionnée avec lexique382 pour éviter
les problèmes de duplication de lignes.

```{r}
### Code illustratif à ne pas lancer ###

# Tous les tokens qui n'ont pas été lémmatisés
#res.lemmat.NA <- res.lemmat %>% filter_at(vars(lemma.x,lemma.y,lemma),all_vars(is.na(.)))

# On enregistre que la colonne token
#res.lemmat.NAB <- data.frame(x=unique(res.lemmat.NA$token))  
# write.csv2(x=res.lemmat.NAB,"data/lemma_complet.csv",fileEncoding = "latin1") # exportation en CSV 

### Code à lancer ###

lexique <- mixr::get_lexicon(language = "fr")

#Importation de notre base de lemmatisation

lemma_complet <- read.csv(file = "data/lemma_complet.csv", fileEncoding = "latin1", header = T, sep = ";")
lemma_complet[lemma_complet== ""] <- NA

#Fusion des 2 bases
lexique382 <- rbind(lexique382, lemma_complet)
lexique382_unique <- lexique382[!duplicated(lexique382$word), ] #supp des doublons : champ, recul, recréer, chalonnais en doubles

#Lemmatisation avec lexique382 + notre base
res.lemmat <- left_join(x=res.lemmat, y = lexique382_unique, by = join_by(x$token==y$word), keep = F, 
                        relationship = "many-to-one")

```

On répète la même chose avec notre dernière base de lemmatisation.

```{r}
lemma3 <- read.delim("data/lemmatization-fr.txt", header = TRUE, stringsAsFactors = FALSE)
colnames(lemma3) <- c("lemma","token")
lemma3_unique <- lemma3[!duplicated(lemma3$token), ]

res.lemmat <- left_join(x= res.lemmat, y = lemma3_unique, by = join_by(x$token==y$token), 
                        keep = F, relationship = "many-to-one")
```

Après cela on obtient un tableau de données avec 6 colonnes. - lemma.x
correspond à hash_lemma_fr - lemma.y à lexique382 - lemma à notre base de github

```{r}
head(res.lemmat)

```

En regardant les lemmatisation proposées, nous avons conclus que lexique382
était la meilleure base que nous avions, puis hash_lemma_fr et enfin la base
trouvée sur GitHub. Nous avons donc réalisé une fonction pour fusionner en
prenant en compte cet ordre de priorité.

```{r}

tri.reslemmat <- function(base) {
  
  # créer une nouvelle colonne lem.f contenant le lemma disponible 
  # d'abord on regarde si il y a qqch dans lemma y : prio n°1
  base$lem.f <- ifelse(test = !is.na(base$lemma.y), yes = base$lemma.y,
                       
                       # sinon : on va prendre le lemma de lemma x 
                       no = ifelse(test = !is.na(base$lemma.x), yes = base$lemma.x,
                                   
                                   # sinon : on va prendre le lemma de lemma 
                                   no = ifelse(test = !is.na(base$lemma), yes = base$lemma,
                                               
                                               # sinon on met un na    
                                               NA)))
  
  base = base[,c(1,7)]  # on conserve les colonnes : doc et lem final 
  base = drop_na(base) # on retire les lignes avec NA
  
  return(base)
}


res.lemmat <- tri.reslemmat(base = res.lemmat)
```

A la fin nous obtenons un tableau avec 2 colonnes :

-   le numéro du document

-   les lemmatisation de chaque token de chaque document

# Topic Modelling

On souhaite appliquer un algorithme de topic modelling (ici Latent Dirichlet
Association) sur les descriptions prétraitées selon la procédure précédemment
décrite.

On fait l'hypothèse que les description des PAT abordent différents topics, que
ces topics sont composés de mots, et que chaque description n'est pas composée
de la même proportion de topics.

A ce stade, on se demande si les topics que l'on peut extraire correspondront
aux 9 axes thématiques de nos PAT: Gouvernance Économie alimentaire Culturel et
gastronomie Éducation alimentaire Nutrition et santé Justice sociale
Environnement Restauration collective Urbanisme

La méthode étant supervisée, il faudra essayer d'optimiser les paramètres de
l'algorithme (coefficients alpha, beta) ainsi que le nombre de topic (k).

Plus le coefficient alpha est petit, mieux les topics seront séparés, car les
termes représentatifs de chaque topic seront plus caractéristiques.

```{r, message=F, warning=F}
library(topicmodels)  # For LDA
library(tm)          # For DocumentTermMatrix
library(slam)
library(ggplot2)
library(FactoMineR)

```

Il faut créer à partir de notre dataframe res.lemmat un data frame contenant
trois colonnes : "text", "mot" et "fréquence", contenant la fréquence
d'apparition de chaque mot dans chaque texte.

```{r}
# df avec les mots regroupés par fréquence 
df_lda <- res.lemmat %>%
  group_by(doc, lem.f) %>%
  summarise(freq = n(), .groups = 'drop')
```

Puis on créé 'stm', un objet de la classe 'simple triplet matrix' sur lequel la
LDA pourra être réalisée.

```{r}
# Get unique documents and terms
docs <- unique(df_lda$doc)     # noms des documents
terms <- unique(df_lda$lem.f)  # liste des termes  

# Create mappings
doc_to_idx <- setNames(seq_along(docs), docs)
term_to_idx <- setNames(seq_along(terms), terms)

# Map to indices
i <- as.integer(doc_to_idx[df_lda$doc])
j <- as.integer(term_to_idx[df_lda$lem.f])
v <- as.integer(df_lda$freq)  # integer

# Create simple_triplet_matrix
stm <- simple_triplet_matrix(
  i = i, 
  j = j, 
  v = v,
  nrow = length(docs),
  ncol = length(terms),
  dimnames = list(docs, terms)
)

# save(stm, file = 'stm.RData')
```

```{r}
set.seed(1234)
# on fixe la graine 
```

On créé une fonction qui va réaliser la LDA à partir de k et alpha

```{r}
lda.fit <- function (k = k, alpha = 50/k) {
  set.seed(1234)
  lda_model <- LDA(stm, k = k, method = "Gibbs", control = list(alpha =alpha,seed = as.integer(500)))

  tidy_model_beta <- tidy(lda_model, matrix = "beta")
  # coefficients beta
  # => proba que chaque mot appartienne à un topic (1 des k topics)
  # Les mots avec le coeff beta le + élevé de chaque topic sont ceux qui participent le + à caractériser chaque topic 
  
  plot <- tidy_model_beta %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_viridis_d() + 
  coord_flip() + 
  labs(x = "Topic", 
       y = "beta score", 
       title = "Topic modeling of PAT description")
  
  print(list(plot,tidy_model_beta))
}
```

Voici les quelques tests que nous avons réalisé et les interprétations obtenues
:

Avec 9 clusters :

```{r}
axes9 <- lda.fit(k = 9) 
axes9[[1]]
doc_analyse <- axes9[[2]]
write.csv(axes9[[2]],file="resume2.csv")
```

Interprétation : Nous avons essayé de nommer nous mêmes les topics, soit en
utilisant les axes thématiques (si cela peut correspondre à un axe thématique),
soit en utilisant des LLM nous fournissant des noms à partir des mots
caractérisant ces topics.

Pour 9 topics :

-   Restauration collective \| =\> Restauration collective

-   Agriculture & Climat \| =\> Environnement ?

-   Production alimentaire locale \| =\> Economie alimentaire

-   Ruralité \| =\> Urbanisme ?

-   Production agricole \| =\>

-   Développement territorial \| =\> Gouvernance

-   Agroéconomie \| =\> Economie Alimentaire

-   Alimentation durable \| =\> Nutrition et santé ?

-   Qualité alimentaire / Accès \| =\> Justice sociale

Education alimentaire ? semble absent ; aussi Culturel et gastronomie

Nous essayons avec k = 8

```{r}
axes8 <- lda.fit(k = 8) 

```

identificaton des clusters / axes :

-   Accessibilité / Justice Sociale

-   Restauration collective (+ Nutrition santé + )

-   Ruralité (urbanisme ?) ?

-   Production agricole

-   Climat & Agriculture

-   Gouvernance

-   Economie alimentaie

Education alimentaire ? Culturel et gastronomie ? Nutrition ?

Et avec 7 topics ?

```{r}
axes7 <- lda.fit(k = 7)
```

Interprétation des clusters :

-   Production agricole

-   Economie alimentaire

-   mix entre Nutrition santé et Justice Sociale ?

-   Agro-environnement \| =\> Environnement

-   Développement territorial \| =\> Gouvernance

-   Ruralité \| =\> Urbanisme

-   Restauration collective ?

On se demande au final s'il faut garder dans le modèle "alimentaire",
"alimentation", car ils apparaissent très fréquemment et dans plusieurs thèmes.
Cela donne une information intéressante que l'on perdrait si on les supprimait :
exemple, ce thème est lié à une consommation durable (ex : alimentaire
alimentation local durable) et moins présent quand on parle par exemple de
climeat ("agricole enjeu climatique ressource..."). Cependant, cela donne un
fort déséquilibre car ces mots ont une fréquence naturellement forte dans la
description des PAT.

On essaye avec k = 10

```{r}
axes10 <- lda.fit(k = 10)
```

1 - Nutrition santé

2 — Environnement ?

3 — Culture et gastronomie

4 - Gouvernance

5 — Resto collective

6 - Environnement ?

7 — Ruralité (urbanisme)

8 - Production agricole

9 - Alimentation durable

10 - Marchés / Economie Alimentaire

Cette interprétation est difficile et semble très variables des paramètres du
modèle. Nous avons aussi essayer de faire varier alpha (avec k = 9)

```{r}
lda.fit(k = 9, alpha = 0)
```

```{r}
lda.fit(k = 9, alpha = 0.5)
```

thèmes identifiés :

-   Gouvernance

-   Ruralité

-   Economie alimentaire

-   Production agricole

-   Filière et circuits locaux

-   Environnement (agriculture durable)

-   Alimentation durable

-   Justice Sociale - Nutrition santé

-   Culturel et gastronomie

```{r}
lda.fit(k = 9, alpha = 1)
lda.fit(k = 9, alpha = 2)

```

Pour conclure, on peut dire qu'il n'est pas extrêmement simple d'essayer de
comprendre les thématiques qui sont abordées dans les descriptions des enjeux
des PAT, notamment :

\- car certains mots, comme 'alimentaire' 'alimentation' 'territoire' sont très
fréquents

\- car certains thèmes sont imbriqués (justice sociale et santé par exemple, ou
restauration collective et santé)

\- certains thèmes sont cependant peu sensibles aux variations de paramètres (ex

:   gouvernance, production agricole, etc)

Un travail d'interprétation est donc nécessaire afin de comprendre combien de
topics garder, et comment interpréter ceux-ci.

# Jusqu'à une classification ?

On peut essayer d'extraire de notre résultat de topic modelling les vecteurs de
proportions de chaque topic abordé dans chaque PAT : il s'agit des coefficients
gamma. Il y a 1 gamma par topic et par texte =\> cela corresond au % du topic
abordé dans chaque texte et donc la probabilité de chaque document d'appartenir
à un topic.

On va ensuite essayer de faire une classification des PAT selon ces vecteurs de
proportion.

```{r}
model.lda <- LDA(stm, k = 9, method = "Gibbs",  control = list(alpha =0.5))
tidy_model_gamma<- tidy(model.lda, matrix = "gamma") # on extrait les gamma de ce modèle 

tidy_model_gamma <- data.frame(tidy_model_gamma) # on les transforme en dataframe

tidy_model_gamma_wide <- tidy_model_gamma %>% pivot_wider(
                               names_from = topic, 
                               values_from = gamma)
 

tidy_model_gamma_wide$document <- gsub("text", "", tidy_model_gamma_wide$document)
tidy_model_gamma_wide$document <- as.numeric(tidy_model_gamma_wide$document)
tidy_model_gamma_wide[,c(2:10)] <- 100*round(tidy_model_gamma_wide[,c(2:10)],3)

head(tidy_model_gamma_wide)

```

On réalise ensuite une PCA sur nos vecteurs puis une classification hiérarchique
ascendante sur les PAT décrits par leurs vecteurs de proportion de chaque topic
:

```{r}
res.pca <- PCA(tidy_model_gamma_wide, scale.unit = T, quali.sup = 1, ncp = 3, graph = F)
# barplot(res.pca$eig[,2]) # scree plot => NCP = 3

plot.PCA(x = res.pca, choix = "ind", invisible = "quali", label = "none")

res.hcpc <- HCPC(res = res.pca, nb.clust = -1, description = T)

clust <- data.frame(cbind(doc = tidy_model_gamma_wide$document, 
                          res.pca$ind$coord[,1:2],
                          clust = res.hcpc$data.clust$clust))
clust$clust <- as.factor(clust$clust)

head(clust)

res.hcpc$desc.var
```

On représente graphiquement nos PAT selon leur cluster. On pourrait ajouter de
l'information illustrative pour essayer de comprendre cette typologie (si les
topics ont du sens)

```{r}
# graph à améliorer 
clust %>% ggplot() +
  aes(x = Dim.1, y = Dim.2, group = clust, col = clust) +
  geom_point() +
  ggtitle("Typologies des PAT selon leurs description") +
  theme(
    title = element_text(size = 14, hjust = 0.5),
    text = element_text(size = 12)
  )

```

On essaye de représenter un "PAT" moyen de chaque cluster. Par quel topic est il
le plus concerné ? Quel topic est moins abordé ? Si les topics ont du sens, ce
type de représentation nous permettent de comprendre nos PAT à travers les
thématiques qui qualifient les enjeux liés à leurs territoires.

attention : les set seed ne fonctionnent pas donc les thèmes bougent donc ce
graph n'a pas les bonnes légendes =\> voir celui plus bas.

```{r}
tidy_model_gamma_t <- cbind(tidy_model_gamma_wide, clust = clust$clust )


pat_moyen <- tidy_model_gamma_t %>% 
  group_by(clust) %>% 
  summarize(across(2:10, mean, .names = "mean_{.col}"))

library(tidyr)
pat_moyen_long <- pivot_longer(
  data = pat_moyen,
  cols = 2:10,
  names_to = c("mean", "topic"),
  names_sep = "_",
  values_to = "proportion"
)

pat_moyen_long %>% ggplot() +
  aes(x = clust, y = proportion) +
  geom_col(aes(fill = topic), position = position_stack()) +
  # scale_color_manual(values = c("green4","blue4","lightgreen","grey","orange2", "purple2","yellow3","green2","pink"))+
  scale_fill_manual(
    values = c("yellow2","blue4","lightgreen","grey","orange2", "purple2","yellow3","green2","pink"),
    labels = c(
      "Production agricole",
      "Gouvernance",
      "Alimentation durable",
      "Ruralité",
      "Restauration Collective",
      "Justice Sociale",
      "Filières locales",
      "Environnement",
      "Nutrition et santé"
    )
  )

```

(le set seed ne fonction pas très bien donc les sujets sont bousculés à chaque
nouveau modèle donc ce graphique n'est pas forcément interprétable en l'état
mais c'est une idée de résultat que l'on peut obtenir ) : voici ci dessous une
capture d'écran du même code exécuté *avec les bonnes légendes*

![Texte Alternatif](images/prop.jpg)
Filtre des PAT avec trop petites desc (< 20 lemmes)
```{r}
res.lemmat <- res.lemmat %>% 
  group_by(doc) %>%
  mutate(n_lignes = n()) %>% 
  filter(n_lignes >= 20)

# on retire 11 PAT 
# length(unique(res.lemmat$doc))
# length(unique(res.lemmat2$doc))

```



```{r}
# sauver le res.lemmat dans l'environnement 

save(res.lemmat, file = "data/res.lemmat.RData")

```



